{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"logo.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Máquinas de Vectores Soporte (Support Vector Machines, ó SVM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a ver como utiliar el algoritmo SVM en `scikit-learn`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "matplotlib.rcParams['figure.figsize'] = [10, 10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cargamos los datos\n",
    "\n",
    "En este caso vamos a usar un dataset nuevo, el dataset [Iris](https://archive.ics.uci.edu/ml/datasets/iris), que contiene 4 características sobre 3 tipos distintos de variedades de la flor Iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "\n",
    "datos = load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(datos.DESCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datos.target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datos.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = pd.DataFrame(datos.data, columns=datos.feature_names)\n",
    "\n",
    "iris[\"objetivo\"] = datos.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variables_indep = iris.drop(\"objetivo\", axis=1).columns\n",
    "\n",
    "iris_X = iris[variables_indep]\n",
    "iris_y = iris[\"objetivo\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "iris_X_train, iris_X_test, iris_y_train, iris_y_test = train_test_split(iris_X, iris_y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC, SVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimador_svm =  SVC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimador_svm.fit(iris_X_train, iris_y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimador_svm.predict(iris_X_test)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **C** es el parámetro de Coste (que regula el impacto de las variables de holgura y ayuda a regularizar el modelo.\n",
    "- **kernel** indica que kernel usar (rbf, radial basis function por defecto). Se puede usar cualquier kernel definido por nosotros, por defecto SVC conoce `rbf`, `poly` (polinomial), `linear` (lineal) o `sigmoid` (sigmoide).\n",
    "- **class_weight**, nos permite pasar un diccionario de la forma `{clase:peso}` que permite asignar más peso a una clase que a otra. Para problemas con clases no balanceadas, podemos usar el parámetro 'balanced' para que se ajusten los pesos en función del número de casos de cada clase. \n",
    "- **decision_function_shape** si usar una estrategia de uno contra uno (ovo) o uno contra todos (one versus rest, ovr) en casos de clasificación multiclase.\n",
    "- **cache_size** es el tamaño (en megabytes) del caché del modelo (cuantos datos puede guardar en memoria y reutilizarlos sin tener que calcularlos). SVMs son computacionalmente complejos asi que si hay mas memoria disponible mejor incrementarl este valor(por ejemplo, a 1000mb o 2000mb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El parámetro `support_vectors_` nos devuelve los vectores soporte."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "estimador_svm.support_vectors_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El parámetro `n_support_` nos dice cuantos vectores soporte (es decir, puntos tocando el margen del hiperplano de decisión) existen por clase, esto nos da una medida de cuan faciles son de separar cada clase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimador_svm.n_support_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kernels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a ver el efecto que tienen diferentes kernels en la creación del hiperplano de decisión.\n",
    "\n",
    "Para verlo en un gráfico de dispersión tomamos solo las dos primeras variables del dataset (longitud y grosor del sépalo)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = datos.data[:, :2]\n",
    "y = datos.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install mlxtend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.plotting import plot_decision_regions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En primer lugar tenemos el kernel lineal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimador_svm_lineal = SVC(kernel=\"linear\")\n",
    "estimador_svm_lineal.fit(X, y)\n",
    "\n",
    "plot_decision_regions(X, y, clf=estimador_svm_lineal);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El kernel polinomial calcula el producto escalar de dos vectores en un espacio dimensional donde tiene en cuenta las combinaciones polinómicas de los mismos. Esto es, si tenemos dos vectores V1 y V2 de la forma `[x1, x2]` el kernel polinomial va a producir un resultado similar a hacer un producto escalar de V1 y V2 pero transformados como `[x1, x2, x1^2, x1x2, x2^2...]`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El kernel polinomial tiene el hiperparámetro `degree` que indica el grado de expansión polinómica (esto es, el grado de las combinaciones de las variables que queremos tener en cuenta). Por defecto es 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimador_svm_polinomial = SVC(kernel=\"poly\")\n",
    "estimador_svm_polinomial.fit(X, y)\n",
    "\n",
    "plot_decision_regions(X, y, clf=estimador_svm_polinomial);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ver como varia la frontera de decisión en función de los grados de expansión. Cuantos más grados más complejo podrá ser el hiperplano. Con grado 1 se convierte en un kernel lineal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimador_svm_polinomial = SVC(kernel=\"poly\", degree=1).fit(X, y)\n",
    "plot_decision_regions(X, y, clf=estimador_svm_polinomial);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimador_svm_polinomial = SVC(kernel=\"poly\", degree=2).fit(X, y)\n",
    "plot_decision_regions(X, y, clf=estimador_svm_polinomial);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimador_svm_polinomial = SVC(kernel=\"poly\", degree=6).fit(X, y)\n",
    "plot_decision_regions(X, y, clf=estimador_svm_polinomial);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El kernel polinomial tiene también el parámetro que lo regula, alpha en la ecuación de arriba, aunque en la implementación de scikit-learn se llama `gamma` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimador_svm_polinomial = SVC(kernel=\"poly\", degree=3, gamma=0.1).fit(X, y)\n",
    "plot_decision_regions(X, y, clf=estimador_svm_polinomial);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos que un gamma bajo reduce la complejidad del kernel polinomial convirtiendolo prácticamente en lineal. Si no se especifica se usa `gamma=1/n_variables`, o sea en este caso 1/2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El kernel gausiano (radial basis function, o rbf) hace una transformacion radial (esto es, en funcion de la distancia de los puntos al origen)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimador_svm_rbf = SVC(kernel=\"rbf\")\n",
    "estimador_svm_rbf.fit(X, y)\n",
    "\n",
    "plot_decision_regions(X, y, clf=estimador_svm_rbf);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos probar como varia la frontera de decisión en función de gamma:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimador_svm_rbf = SVC(kernel=\"rbf\", gamma=0.1).fit(X, y)\n",
    "plot_decision_regions(X, y, clf=estimador_svm_rbf);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimador_svm_rbf = SVC(kernel=\"rbf\", gamma=10).fit(X, y)\n",
    "plot_decision_regions(X, y, clf=estimador_svm_rbf);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimador_svm_rbf = SVC(kernel=\"rbf\", gamma=100).fit(X, y)\n",
    "plot_decision_regions(X, y, clf=estimador_svm_rbf);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A mayores valores de `gamma` mayor es la capacidad del kernel rbf de crear segmentos alrededor de los datos. \n",
    "Vemos que para `gamma=100` el modelo está sobreajustando mucho (creando pequeñas burbujas alrededor de cada observación)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parámetro de coste C\n",
    "\n",
    "El parámetro C nos da una medida de como queremos penalizar al modelo cuando clasifica un ejemplo de forma errónea y es la manera de regularizar los modelos SVM. Valores altos de C permiten controlar la complejidad del modelo (evitando el sobreajuste) a coste de no clasificar bien un porcentaje de los ejemplos en los datos de entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import validation_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rango_c = np.linspace(0.01,50,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimador_svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_scores, test_scores = validation_curve(estimador_svm, iris_X, iris_y, param_name=\"C\",\n",
    "                                             param_range=rango_c, cv=10, scoring=\"f1_weighted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_scores_mean = np.mean(train_scores, axis=1)\n",
    "test_scores_mean = np.mean(test_scores, axis=1)\n",
    "\n",
    "plt.plot(rango_c, train_scores_mean, 'o-', color=\"r\",\n",
    "             label=\"Funcionamiento datos_entrenamiento\")\n",
    "plt.plot(rango_c, test_scores_mean, 'o-', color=\"g\",\n",
    "             label=\"Funcionamiento Validación Cruzada\")\n",
    "plt.title(\"Curva Validación: SVM con kernel rbf\")\n",
    "plt.xlabel(\"Constante de regularización (C)\")\n",
    "plt.ylabel(\"Puntuación F1\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos que conforme aumenta C, el modelo sobreajusta más (ya que mejora su funcionamiento en los datos de entrenamiento pero empeora en los de test)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Probabilidades\n",
    "\n",
    "Los modelos SVM no proporcionan probabilidades (por que no hacen inferencia estadística en ese sentido, sino que funcionan de un modo geométrico), por eso por defecto el modelo `SVC` no proporciona el metodo `predict_proba` que hemos visto en otros estimadores (regresión logística por ejemplo).\n",
    "\n",
    "Sin embargo, la implementación de sklearn permite pasarle el parámetro `probability=True` que calcula de forma adicional las probabilidades usando escalado de Platt (básicamente, entrena una regresión logística en las distancias al hiperplano computadas por el SVM).\n",
    "\n",
    "Éste método es computacionalmente complejo, y además tiene ciertos fallos teóricos (por ejemplo, puede haber casos en los que se prediga una clase en un problema de clasificación binaria y que su método `predict_proba` produzca una probabilidad menor que 0.5). \n",
    "\n",
    "Para aquellos casos que se necesite una forma de puntuar nuevas observaciones pero que dicha puntuacion no tenga que ser una probabilidad es mejor usar directamente el output de la función de decisión con `decision_function`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimador_svm =  SVC()\n",
    "\n",
    "estimador_svm.fit(iris_X_train, iris_y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimador_svm.predict(iris_X_test)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "estimador_svm.decision_function(iris_X_test)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimador_svm.predict_proba(iris_X_test)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimador_svm_prob =  SVC(probability=True)\n",
    "estimador_svm_prob.fit(iris_X_train, iris_y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimador_svm_prob.predict_proba(iris_X_test)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Vemos que usando `probability=True` se pueden calcular las \"probabilidades\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
